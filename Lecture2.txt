Intro
0:01
and we're recording as well okay great just to remind you again hello uh we're recording the classes so if you're
0:06
uncomfortable speaking in in the camera you're not in the picture but your voice might be uh on the on the uh recording
0:14
okay great uh as you can see also the screen is uh wider than it should be and I'm not sure how to fix it so we'll have
0:20
to live with it luckily your visual cortex is very good it's very invariant to stretching so this is not a problem
Assignments
0:27
okay so we'll start out with some administrative things before we dive into the class um the first assignment will come out
0:33
tonight or early tomorrow uh it is due on January 20 so you have exactly two weeks you will be writing a kous neigh
0:40
classifier a linear classifier and a small two layer neural network and you'll be writing the entirety of back propagation algorithm for a two two
0:46
layer neural network we'll cover all that material in the next two weeks and
0:51
um Warning by the way there are assignments from last year as well and we're changing the assignments so they will please do not complete a 2015
0:58
assignment that's something to be aware of and uh for your computation by the way we'll be using Python and numpy uh
1:06
and we'll also be offering terminal.com which is uh which is basically these virtual machines in the cloud that you
1:11
can use if you don't have a very good laptop and so on I'll go into detail of that in a bit I just like to point out
Python
1:16
that for the first assignment we assume that you'll be relatively familiar with python and you'll be writing these
1:22
optimized numpy Expressions where you're manipulating these matrices and vectors in very efficient forms so for example
1:28
if you're seeing this code and it's doesn't mean anything to you then please have a look at our python NPI tutorial
1:33
that is up on the website as well it's written by Justin and it's very good and uh so go through that and familiarize
1:38
yourself with the notation because you'll be seeing you'll be writing a lot of code that looks like this uh where we're doing all these optimized
1:45
operations so they're fast enough to run on a CPU now in terms of terminal basically what this amounts to is that
Terminal
1:51
we'll give you a link to the assignment you'll go to a web page and you'll see something like this this is a virtual
1:56
machine in the cloud that has been set up with all the dependencies off the assignment they're all installed already all the data is already there and so you
2:03
click on launch a machine and this will basically bring you to something like this this is running in your browser and
2:09
this is basically a a thin layer on top of an AWS uh machine a UI layer here and
2:15
so you have an iPad to notebook and a little terminal and you can go around and this is just like a machine in the cloud and so they have some CPU
2:21
offerings and they also have some GPU machines that you can use and so on uh you normally have to pay for terminal but we'll be Distributing credits to you
2:28
so you just uh email us to a specific ta that will decide in a bit you email to a TA and you ask for money we'll send you
2:34
money and we keep track of how much money we've sent to all the people so you have to be responsible with the funds uh so this is also an option for
2:41
you to use if you like uh okay any details about this y go
2:48
a quick question you said that they have GP units does that mean we can write Huda code or is that not acceptable uh
2:54
you can you can write it if you like it's not required for the assignment but you can probably get that to run yeah
3:01
okay great so I'm just going to dive into the lecture now today we'll be talking about uh image classification
Image Classification
3:06
and especially uh we'll start off on linear classifiers so when we talk about image classification um the basic task
3:13
is that we have some number of fixed categories say a dog cat truck plane or so on we get to decide what these are
3:19
and then the task really is to take an image which is a giant grid of numbers and we have to transform it to one of
3:24
these labels we have to bin it into one of the categories this is the image classification problem we'll spend most of our time talking about this one
3:30
specifically but if you'd like to do any other task in computer vision such as object detection image captioning segmentation or whatever else you'll
3:37
find that once you know about image classification and how that's done everything else is just a tiny Delta on
3:42
top of it so you'll be in a great position to do any of the other tasks so it's really good for conceptual understanding and we'll work through
3:48
that as a specific example uh to simplify things in the beginning now why is this problem hard just to give you an
Semantic Gap
3:54
idea the problem is what we refer to as a semantic Gap this image here is a giant grid of numbers the way images are
4:00
represented in the computer is that this is basically say roughly a 300X 100x 3 pixel array so threedimensional array
4:07
and the three is for the three color channels red green and blue and so when you zoom in on a part of that image it's
4:12
basically a giant grid of numbers between 0 and 255 uh so that's what we
4:17
have to work with these numbers indicate the amount of brightness in all the three color channels at every single position in the image and so the reason
4:24
that image classification is difficult is when you think about what we have to work with these like millions of numbers
4:29
of that form and having to classify things like cats it quickly becomes apparent the complexity of the task so
Classification Challenges
4:36
for example the camera can be rotated around this cat and it can be Zoomed In and Out and rotated shifted the focal
4:43
properties intrinsics of that camera can be different and think about what happens to the brightness values in this
4:48
grid as you actually do all these Transformations with a camera they'll completely shift all the patterns are changing and we have to be robust to all
4:54
of this there's also many other challenges for example challenges of Illumination here we have have a long
Illumination Challenges
5:00
cat uh long cat we actually have two of them but you almost even can't see the other one um and so one cat is basically
5:07
illuminated quite a bit and the other is not but you can still recognize two cats and so think about again the brightness
5:13
valys on the level of the grid and what happens to them as you change all the different lightings and all the possible
5:18
lighting schemes that we can have in the world we have to be robust to all of that there's issues of um deformation
Deformation Challenges
5:25
many classes lots of uh strange uh Arrangements of all these objects we'd
5:31
like to recognize so cats coming in very different uh poses by the way the slides when I create them they're quite dry
5:37
there's a lot of math and so on so this is the only time I get to have fun so uh that's why I just pile everything with
5:42
cat pictures uh so we have to be robust to all of these deformations you can still recognize that there's a cat in all of these images despite its
5:48
Arrangement there's problems of occlusion so sometimes we might not see the full object but you still recognize
Occlusion Challenges
5:54
that that's a cat behind a curtain that's a cat behind a water bottle and there's a also a cat there inside a
6:00
couch even though you're seeing just tiny pieces pieces of this uh class basically there's problems of background
6:06
clutter so things can blend into the environment we have to be robust to that and there's also what we refer to as
6:13
intraclass variation so cat actually there's a huge amount of cats uh just species uh and so they can look
6:19
different ways we have to be robust to all of that so I just like you to appreciate the complexity of the task when you consider any one of these
6:25
independently is difficult but when you consider the full cross product of all these different things and the fact that
6:31
our algorithms have to work across all of that it's actually quite amazing that anything works at all uh in fact not
6:37
only does it work but it works really really well almost at human accuracy we can recognize thousands of categories like this and we can do that in a few
6:43
dozen milliseconds uh with the current technology and so that's what you'll learn about in this class um so what
6:49
does an image classifier look like basically we're taking this 3D array of pixel values we'd like to produce a
6:54
class label and what I'd like you to notice is that there's no obvious way of actually encoding any of this of these
7:00
classifiers right there's no simple algorithm like say you're taking an algorithm class in your early computer science curriculum you're writing bubble
7:07
swort or you're writing something else to do any particular task you can Intuit all the possible steps and you can enumerate them and list them and play
7:13
with it and analyze it but here there's no algorithm for detecting a cat under all these variations or it's extremely
7:19
difficult to think about how you'd actually write that up what is the sequence of operations you would do or an arbitrary image to detect a cat
7:25
that's not to say that people haven't tried especially in the early days of computer vision uh there were these explicit approaches as I'd like to call
Explicit Approaches
7:31
them where you think about okay a cat say is a we' like to maybe look for um
7:36
little earpieces so what we'll do is we'll detect all the edges we'll Trace out edges we'll classify the different
7:42
shapes of edges and their Junctions we'll create you know libraries of these and we'll try to find their arrangements
7:47
and if we ever see anything earlike then we'll detect a cat or if we see any particular texture of some particular frequencies we'll detect a cat and so
7:54
you can come up with some rules but the problem is that once I tell you okay I'd like to actually recognize a boat now or a person then
8:01
you have to go back to the drawing board and you have to be like okay what makes a boat exactly what's the arrangement of edges right it's completely unscalable
8:07
approach to the to classification and so the approach we're adopting in this class and the approach that works much better is uh the data driven approach
Datadriven Approaches
8:15
that we like in the framework of machine learning and uh just to point out that in these days actually in the early days
8:20
they did not have the luxury of using data because at this point in time you're taking you know great scale images of very low resolution you have
8:27
five images and you're trying to recognize things it's obvious not going to work but with the availability of Internet huge amount of data I can
Training Data
8:33
search for example for cat on uh Google and I get lots of cats everywhere and we know that these are cats based on the
8:39
surrounding text in the web pages and so that gives us lots of data so the way that this now looks like is that we have
8:45
a training phase where you give me lots of training examples of cats and you tell me that they're cats and you give
8:51
me lots of examples of any type of other category you're interested in I do I go away and I train a Model A model is a
8:58
class and I can then use that model to actually classify new test data so when I'm given a new image I can look at my
9:04
training data and I can do something with this based on just a pattern matching um and statistics and so on so
9:11
as a simple uh first example we'll work with in this framework consider the nearest neighbor classifier the way
9:16
nearest neighbor classifier works is that effectively we're given this giant training set what we'll do at training time is we'll just remember all of the
9:22
training data so I have all the train data I just put it here and I remember it now when you give me a test image
9:28
what we'll do is we'll compare that test image to every single one of the images we saw in the training data and we'll
9:34
just transfer the label over so I'll just look through all the images um
9:39
we'll work with specific case as I go through this I'd like to be as concrete as possible so we'll work with a specific case of something called cart
CFR10 Dataset
9:44
10 data set the C4 10 data set has 10 labels these are the labels there are
9:50
50,000 training images that you have access to and then there's a test set of 10 10,000 images where we're going to
9:56
evaluate how well the classifier is working and these images are quite tiny they're just little toy data set of 32x32 little thumbnail images so the way
10:04
nearest neighbor classifier would work is we take all this training data that's given to us 50,000 images now at test
Test Images
10:09
time suppose we have these 10 different examples here these are test images along the First Column here what we'll
10:15
do is we'll look up nearest Neighbors in the training set of things that are most similar to every one of those images
10:21
independently so there you see a ranked list of images that are most similar to
10:26
uh in the training data to any one of those 10 uh to every one of those test images over there so in the first row we
10:32
see that there's a truck I think as a test image and there's quite a few images that look similar to it uh we'll
10:37
see how exactly we Define similarity in a bit but you can see that the first Retreat result is in fact a horse uh not
10:43
a truck and that's because of just the arrangement of the blue sky that was throwing that off so you can see that
10:48
this will not probably work very well now how do we Define the distance metric how do we actually do the comparison
Manhattan Distance
10:54
there's several ways one of the simplest ways might be uh a Manhattan distance so an L1 distance or Manhattan distance
11:00
I'll use the two terms interchangeably simply what it does is you have a test image you're interested in classifying
11:05
and consider one single training image that we want to compare this image too basically what we'll do is we'll
11:10
elementwise compare all the pixel values so we'll form the absolute value differences and then we just add all of
11:16
it up so we're just looking at every single Pixel position we're subtracting it off uh and seeing what the
11:21
differences are at every single spatial position adding it all up and that's our similarity so these two images are
11:27
456 different okay so we'll get a zero if we have identical images here just to
11:33
show you code specifically uh the way this would look like this is a full implementation of a nearest neighbor classifier in numpy and
Code
11:39
python um where I filled in the actual body of the two methods that I talked about and basically what we do here at
11:45
training time is we're given this data set X the images and Y which usually denote the labels so we're giving images
11:51
and labels all we do is just just assigned to the class instance methods so we just remember the data nothing is
11:57
being done at predict time though uh what we're doing here is uh we're getting new um test set of images X and
12:05
I'm not going to go through full details but you can see that there's a for Loop over every single test image independently we're getting the
12:11
distances to every single training image and notice that that's only a single line of vectorized python code so in a
12:18
single line of code we're comparing that test image to every single training image in the database and we're Computing this distance in a previous
12:25
slide in a single line okay so that's vectorized code we didn't have to expand out all those four Loops that are
12:30
involved in processing this distance and then we compute the instance that is
12:36
closest so we're getting the Min index so that's the index of the training example that is has the lowest distance
12:42
and then we're just predicting for this image the label of whatever was nearest
12:47
okay so here's a question for you in terms of the nearest neighbor classifier How does its speed depend on the
Question
12:54
training data size what happens is I scale up the training data
13:00
it's slower okay yes it's actually it's in fact linearly slower right because if I have
13:07
I just have to I have to compare to every single training example independently so it's a linear slowdown
13:12
and you'll notice actually as you go as we go through the class is that this is actually backwards because what we really care about in most practical
13:18
applications is we care about the test time performance of these classifiers that means that we want this classifier
13:23
to be very efficient at test time and so there's this tradeoff between really how much compute do we put in the train method and how much Compu do we put in a
13:29
test method a nearest neighbor is instant at train but then it's expensive at test and as we'll see soon comets
13:36
actually flip this completely the other way around we'll see that we do a huge amount of compute at train time we'll be training a convolutional neural network
13:42
but the test Time Performance will be super efficient in fact it will be constant amount of compute uh for every
13:48
single test image we do constant amount of computation no matter if you have a million billion or trillion training
13:54
images i' I'd like to have a trillion tril trillion training images no matter how large your training data set is we
14:00
do a constant compute to classify any single testing example so that's very nice uh practically speaking now I'll
14:06
just like to point out that uh there are ways of speeding up nearest neighbor classifiers there's these approximate
14:11
nearest neighbor methods flan is an example library that people use often in practice that allows you to speed up uh
14:17
this process of nearest neighbor matching but uh that's just a side note
14:23
okay so let's go back to the design of the nearest neb classifier we saw that we've defined this uh distance and I've
14:29
arbitrarily chosen to show you the Manhattan distance which compares the difference of the absolute values
14:34
there's in fact many ways you can formulate a distance metric and so there's many different choices of exactly how we do this comparison
14:41
another simp another choice that people like to use in practice is what we call the ukian or L2 distance which instead
14:46
sums up the differences in the sums of squares of these differences between images and so this
14:53
Choice what happened there did someone push a button over there in the back
15:02
okay thank you so this choice of what how exactly we compute a distance it's a discrete choice that we have control
15:09
over that's something we call the hyperparameter it's not really obvious how you set it it's a hyperparameter we have to decide later on exactly how to
15:15
set this somehow another sort of hyper parameter that I'll talk about in context of nearest neighbor classifier
kNN
15:21
is when we generalize nearest neighbor to what we call a canor neighbor classifier so in a canor neighbor classifier instead of retrieving for
15:27
every test image the single nearest Training example will in fact retrieve several nearest examples and we'll have
15:33
them do a majority vote over the classes to actually classify every test instance so say a five nearest neighbor we would
15:39
be retrieving the five most similar images in the training data and doing a majority vote of the labels here's a
15:45
simple two-dimensional data set to illustrate the point so here we have a three class data set in 2D and here I'm
15:51
drawing what we call decision regions of this nearest neighbor classifier here what this refers to is we're showing the
15:57
training data over there and we're coloring the entire 2D plane by what uh class this nearest neighbor classifier
16:03
would assign at every single point suppose you suppose you had a test example somewhere here then this is just saying that that this would have been
16:08
classified as blue class based on the nearest neighbor you can for example note that here is a point that is a
16:14
Green Point inside the blue cluster and it has its own little region of influence where it would have classified a lot of test points around it as green
16:21
because if any point fell there then that Green Point would have been the nearest neighbor now when you move to higher numbers for K such as five
16:28
nearest neighbor class classifier what you find is that the boundaries start to smooth out it's kind of this uh nice uh
16:34
effect where even if there's this one point um kind of randomly as a noise and
16:39
outlier in the blue cluster it's actually not influencing the predictions too much because we're always retrieving five nearest neighbors and so they get
16:46
to overwhelm the green point so uh in practice you'll find that usually kers neighbor classifiers offer better uh
16:52
better performance at test time now but again the choice of K is again a hyper parameter right so I'll come back to
16:59
this in a bit just to show you an example of what this would look like here I'm retrieving 10 most similar
17:04
examples they're ranked by their distance and I would actually do a majority vote over these training examples here to classify every Test
17:10
example here okay so let's do a bit of uh questions
Questions
17:16
here just for fun uh consider what is the accuracy of the neural classifier on
17:22
the training data when we're using ukian distance so suppose our test set is exactly the training data and we're
17:29
trying to find the accuracy in other words how many how often would we get the correct answer 100% 100% good
17:40
why okay among the murmur um yeah that's correct so we're always find a training
17:45
example exactly on top of that test which has zero distance and then it St will be transferred over good uh What if
17:51
we're using the Manhattan distance
17:57
instead so Manhattan distance doesn't use sum of
18:03
squares it uses sum of absolute values of differences would it be the same it would it's just a trick question it
18:08
would be so okay good uh so we're we're keeping
18:14
paying attention here uh okay what is the accuracy of the K nearest neighbor classifier in a training data then it
18:19
say k was 5 is it 100% no not necessarily right good because basically
18:26
the points around you could overwhelm you even if you're uh best example is actually of the of a different class
18:32
okay good so we've discussed two choices of different uh here hyper parameters we have the distance metric it's a hyper
18:38
parameter and this K we're not sure how to set it should be 1 2 3 10 and so on so we're not exactly sure how to set
18:45
these in fact they're problem dependent you'll find that you can't find a consistently best choice for these hyper parameters in some applications some
18:51
case might look might work better than other applications so we're not really sure how to set this so here's an idea
18:57
uh we have to basically try out lots of different type parameters so what I'm going to do is I'm going to take my um
Test data
19:03
train data and then I'm just going to try out lots of different type parameters so I have my test data and I try out k equal 1 2 3 4 5 6 20 100 I try
19:11
out all the different distance metrics and whatever works best that's what I'll take so that will work very well right
19:17
no it won't work very well why is it not going to why is this not a good idea uh
19:22
because you will fail to generalize on on unseen data okay so basically correct
19:30
so basically um yes so test data is your proxy for your generalization of your
19:37
algorithm you should not touch the test data in fact you should forget that you ever have test data so when once you're
19:43
given your data set always set aside the test data pretend you don't have it that's telling you how well your algorithm is generalizing to unseen data
19:49
points and this is important because you're trying to develop your algorithm and then you're hoping to eventually deploy it in in some setting and you'd
19:55
like to have an understanding of exactly how well do I expect this to work in practice right and so uh you'll see that for
20:01
example sometimes you can perform very very well on train data but not generalize very well to test data when you're overfitting and so on a lot of
20:07
this by the way 229 is a requirement for this class so you should be quite familiar with this this is to most extent a um this is kind of more more of
20:14
r view for you but basically this test data is use it very sparingly forget that you have it instead what we do is
20:20
we separate our training data into what we call folds so uh we separate say we use a fivefold validation so we use % of
Crossvalidation
20:29
the training data as a imagine test set data and then we only train on part of it and we test on we test out the
20:35
choices of hyp parameters on this validation set so I'm going to train on my fourfolds and try out all the
20:41
different case and all the different zance metrics and whatever else if you're using approximate nearest neighbor you have many other choices you
20:46
try it out see what works best on that validation data if you're feeling uncomfortable because you have very few training data points people also
20:52
sometimes use cross validation where you actually iterate the choice of your test or validation fold across these choice
20:58
es so I'll first use four one to four for my training and try out on five and then I cycle the choice of the
21:04
validation fold across all the five choices and I look at what works best across all the possible choices of my
21:10
test fold and then I just take whatever works best across all the possible scenarios okay that's referred to as a
21:15
cross validation set as cross validation so in practice the way this would look like say we're cross validating for k
21:21
for a nearest neighbor classifier is we are trying out different values of K and this is our performance um across five
21:29
choices of a fold so you can see that for every single K we have five data points there and then this is the
21:34
accuracy so high is good and I'm plotting a line through the mean and I'm also showing the bars for the standard
21:40
deviations so what we see here is that the performance goes up on the across these validation folds um as you go up
21:47
but at some point it starts to Decay so for this particular data set it seems that k equal to 7 is the best choice so
21:53
that's what I'll use I'll do this for all my hyper parameters also for thism metric and so on I do my cross
21:58
validation I find the best high parameters I set them I fix them then I evaluate a single time on the test set
22:04
and whatever number I get that's what I report as a accuracy of a K classifier on this data set that's what goes into a
22:10
paper that's what goes into a final report and so on that's the final generalization result of what you've
22:16
done um okay any questions about this yep so the reason that happens with K is
22:23
that is that because large K gives you high bias um
22:30
I would be careful with that terminology but basically it's about the statistics of the distribution of these data points in
22:37
your label in your data space and so sometimes it's it's basically hard to
22:42
say like you get where is this picture you see roughly what's happening
22:47
is you get more clunkiness in more case and it just depends on how clunky your data set is that's really what it comes
22:53
down to is uh how how Blobby is it or how specific is it
22:59
I know that's a very handwavy answer but that's roughly what what that comes down to uh so different data sets will have different
23:06
clunkiness Y how do you deal with skewed data sets skewed data sets uh so what is
23:12
that a lot more of one class than other class uh so that's a technical question that I maybe want to get into right now
23:18
but we will address that later in the class probably oh yeah go ahead shoulding
23:23
hyper parameters uh data specifically consider aching uh no not at all because your
23:30
hyper parameters are just choices you're not sure how to set them and different uh different data sets will require
23:36
different choices and you need to see what works best in fact when you try out different algorithms because you're not sure what's going to work best on your
23:41
data the choice of your algorithm is also kind of like a hyper parameter so you're just not sure what works you're
23:47
not different um approaches will give you different generalization boundaries they
23:53
look different and some data sets have different structure than others so some things work better than others and you
23:58
have to just train try it out okay cool I just like to point out
24:04
that K neighbors is no one basically uses this so I'm just going through this just to get you uh used to this approach
24:10
of really how this works with training test splits and so on um the reason this is never used is because first of all
24:16
it's very inefficient but second of all distance metrics on images which are very high dimensional objects they act
24:23
in very unnatural unintuitive ways so here what I've done is we're taking an or image and I change it in three
24:30
different ways but all these three different images here have actually the exact same distance to this one in an L2
24:37
ukian sense and so just think about it this one here is slightly shifted to the left it's basically cropped slightly and
24:44
its distances here are completely different because these pixels are not matching up exactly and it's all introducing all these errors and you're
24:50
getting a distance this one is slightly darkened so you get a small Delta across all spatial locations and this one is
24:55
untouched so you get zero distance errors across everywhere except for in those positions over there and that is
25:01
taking out critical pieces of the image and it doesn't the nearest the nearest nebor classifier would not be able to
25:06
really tell a difference between these settings because it's based on these distances that don't really work very well in this case so uh very unintuitive
25:13
things happen when you try to throw distances on very high dimensional objects that's partly why we don't use this okay so in summary so far uh we're
Summary
25:22
looking at image classification as a specific case and we'll go into different settings later in the class I've introduced enable classifier and
25:29
the idea of having different splits of your data and we have these hyperparameters that we need to pick uh
25:34
and we use cross validation for this usually most of the time people don't actually use entire cross validation they just have a single validation set
25:40
and they try out on the validation set whatever works best in terms of the hyper parameters and once youve get the
25:46
best high parameters you evalate the single time on a test set okay so I'm going to go into linear classification
25:52
bit any questions at this point otherwise great okay
25:59
so we're going to look at linear classification um this is a point where
26:04
we are starting to work towards convolutional Lal networks so there will be a series of lectures we'll start with linear classification that will build up
26:10
to an entire convolutional network analyzing an image now I just like to say that we've motivated the class yesterday from a
26:17
task specific uh view so this class is computer vision class we're interested in you know uh giving machines site
26:24
another way to motivate this class would be from a modelbased point of view in a sense that we're um giving you guys uh
26:30
we're teaching you guys about deep learning and neural networks these are wonderful algorithms that you can apply to many different data domains not just
26:36
Vision so in particular over the last few years we saw that neural networks can not only see that's what you'll
26:42
learn a lot about in this class but they can also hear they're used quite a bit in uh speech recognition now so when you
26:48
talk to your phone that's now a deep neural network they can also uh do machine translation so here you're
26:53
feeding um a neural network a set of words one by one in English and the
26:59
neural network produces the translation in French or whatever else target language you have they can also perform
27:04
control so we've seen neural network applications in um manipul in robots manipulation in playing of Atari games
27:10
so these neural networks learn how to play Atari games uh just by seeing the raw pixels of the screen and we've seen
27:16
neur networks be very successful in a variety of domains and even more than uh
27:21
I put here and we're uncertain exactly where this will take us and then I'd like to also say that we're exploring
27:28
ways for neural networks to think but this is very handwavy it's just a wishful thinking but uh there's some
27:33
hints that maybe uh they can do that as well now neural networks are very nice because they're just a fun modular
27:39
things to play with when I think about working with neural networks I kind of this picture comes to mind for me here
Play with Neural Networks
27:44
we have a neural networks practitioner and she's building what looks to be a roughly 10 layer convolutional neural
27:49
network at this point and so these are very fun really the best way to think about playing with neural networks is
27:55
like Lego blocks you'll see that we're building these little function pieces these Lego blocks that we can stack together to create entire architectures
28:02
and they very easily talk to each other and so we can just create these modules and stack them together and play with
28:07
this very easily um one work that I think exemplifies this is uh my own work
28:13
on image captioning from roughly a year ago uh so here the task was you take an image and you're trying to get the
28:19
neural network to produce a sentence description of the image so for example in the top left these are test set results the neural network would say
28:25
that this is a man in black shirt is playing a guitar or a constructure worker in Orange safety West is working on the road and so on so the neural
28:32
network can look at the image and create this description of every single image and when you go to the details of this
28:38
model the way this works is we're taking a convolutional neural network which we know so there's two modules here in this
Image Captioning Model
28:45
uh system diagram for image captioning model we're taking a convolutional neural network which we know can see and
28:51
we're taking a recurrent neural network which we know is very good at modeling sequences in this case sequences of words that will be describing the image
28:58
and then just as if we were playing with Legos we take those two pieces and we stick them together that's corresponding to this Arrow here in between the two
29:04
modules and these networks learn to talk to each other and in the process of trying to describe the images these
29:10
gradients will be flowing through the convolutional network and the full system will be adjusting itself to better see the images in order to
29:16
describe them at the end and so this whole system will work uh together as one uh so we'll be working towards this
29:22
model we'll actually cover this in class so you'll have full understanding exactly of both this part and this part
29:28
about half halfway through the course roughly and you'll see how that image captioning model works but that's just a motivation for really what we're
29:34
building up to and these are like really nice models to work with Okay but for now back to uh c41 and linear
29:41
classification um so again just to remind you we're working with this data set 50,000 images 10 labels and the way
29:47
we're going to approach linear classification is from what we call a parametric approach K neighbor that we
Nonparametric Approach
29:52
just discussed now is something an instance of what we call non-parametric approach there's no parameters that we're going to be optimizing over this
29:58
distinction will become clear in a few minutes uh so in the parametric approach what we're doing is we're thinking about
30:05
constructing a function that takes an image and produces the scores for your classes right this is what we want to do
30:11
we want to take an image and we'd like to figure out which one of the 10 classes it is so we'd like to write down
30:17
a function an expression that takes an image and gives your do 10 numbers but the expression is not only a function of
30:23
that image but critically it will be also a function of these parameters that I'll call W sence also called the
30:28
weights and so really it's a function that goes from 372 numbers which make up
30:34
this image to 10 numbers that's what we're doing we're defining a function and we'll go through several
30:39
choices of this function in this in the first case we'll look at linear functions and then we'll extend that to get neural networks and then we'll
30:46
extend that to get convolutional neural networks but intuitively what we're building up to is that what we'd like is
30:51
when we pipe this image through our function we'd like the 10 numbers that correspond to the scores of the 10
30:57
classes we'd like the number that corresponds to the cat class to be high and all the other numbers to be low and
31:03
we'll have we don't have a choice over X that X is our image that's given but we'll have Choice over W so we'll be
31:10
free to set this in whatever way we want and we want we'll want to set it so that this function gives us the correct
31:16
answers for every single image in our training data okay that's roughly the approach we're building up towards so
31:21
suppose that we use the simplest uh function arguably the simplest just a linear classification here so X is our
Linear Classification 1
31:28
image in this case what I'm doing is I'm taking this array this image that makes up uh the cat and I'm stretching out um
31:36
all the pixels in that image into a giant column Vector so that X there is a column Vector of 372
31:43
numbers okay and so um if you know your Matrix uh
31:48
Vector operations which you should that's a prerequisite for this class that there is just a matrix multiplication which you should be
31:54
familiar with and basically we're taking X which is a 300 72 dimensional column Vector we're trying to get 10 numbers
32:01
out and it's a linear function so you can go backwards and figure out that the dimensions of this W are basically 10 by
32:06
372 so there are 30,7 7200 numbers that goes into W and that's
32:13
what we have control over that's what we have to tweak and find what works best on our data so those are the parameters
32:18
in this particular case uh what I'm leaving out is there's also an appended plus b sometimes so you have a bias
32:26
these biases are again 10 more par and um we have to also find those so
32:31
usually in your linear classifier you have a w and a b we have to find exactly what works best and this B is not a
32:36
function of the image that's just independent weights on the on How likely any one of those um images might be so
32:43
to go back to your question if you have a very unbalanced data set for uh so maybe you have mostly cats but some dogs
32:50
or something like that then you might expect that the cat the bias for the cat class might be slightly higher because
32:56
by default the classifier wants uh to predict the cat class unless something convinces it
33:02
otherwise something in the image would convince it otherwise okay so to make this more concrete uh I just like to
33:08
break it down but of course I can't visualize it very explicitly with 372 numbers so imagine that our input image
33:13
only had four pixels and imagine so four pixels are stretched out in the column X
33:19
and imagine that we have three classes so red green and blue class or a cat dog ship class okay so in this case W will
33:27
be only a 3x4 Matrix and what we're doing here is we're trying to compute the score of this um image X so this is
33:34
matrix multiplication going on here to give us the output of f which is the scores we get the three scores for three
33:41
different classes so this is an random setting of w just random weights here and we're carrying out the matrix
33:46
multiplication to get some scores so in particular you can see that with this this setting of w is not very good right
33:53
because with this setting of w our cat score 96 is much less than any of the
33:58
other classes right so this was not correctly classified for this training image so that's not it's not a very good
34:04
classifier so we want to change a different we want to use a different W so that that score comes out higher than
34:09
the other ones okay but we have to do that consistently across the entire training set of examples but uh but one
34:17
thing to notice here as well is that basically W it's um this function is in
34:22
parallel evaluating all the 10 classifiers but really there are 10 independent classif iers uh to some
34:29
extent here and every one of these classifiers like say the cat classifier is just the first row of w here right so
34:35
the first row and the first bias gives you the cat score and the dog classifier is the second row of w and the ship
34:42
score the ship classifier the third row W so basically this W Matrix has all these different classifiers stacked in
34:47
rows and they're all being dot producted with the image to give you the scores
34:52
okay so here's a question for you uh what does a linear classifier do in
34:58
English uh we saw the functional form it's taking these images it's doing this funny operation there but what how do we
35:04
really interpret in English somehow what this is
35:10
doing what is this functional form really doing yeah good Ahad if you just think
35:15
of it as a a single binary classifier it's basically drawing a line that will tell you based on the um obser
35:23
observations in your data if the point is below the line and it's not class
35:30
Dimensions okay good so you're thinking about it uh in a spatial domain of X being a high dimensional data point and
35:36
W is really putting uh planes through this High dimensional data point I'll come back to that interpretation in a
35:41
bit uh what other way can we think about this uh for each class it has like sort of a template image that it basically
35:47
multiplies with every image in which everyone comes through the brightest that's okay so you're thinking about it
35:55
more in kind of like a template way where every single one of these rows of w effectively is like this template that
36:01
we're do producting with the image and a do product is really a way of like matching up seeing what what aligns uh
36:08
good um what otherwis yeah I guess it's same as what you said but putting a
36:14
different words uh it's just saying that this part of the image is important
36:19
attribute to be a cat this part of the image uh is not an important not an
36:26
important part as to things like those those kind of yep
36:31
so what you're referring to is that W basically has the capacity to to care or not care about different spatial
36:37
positions in the image right because what we can do is some of the spatial positions in X if we have zero weights
36:43
then the classifier would be doesn't care what's in part of image so if I have zero weights for this part here
36:49
then nothing affects it but for some other parts of the image if you have positive or negative weights something's
36:54
going to happen there and this is going to contribute to the score any other ways of describing it yeah it's taking
37:01
something that exists in image space and projecting it into a space of labels uh yeah so taking yeah so you can
37:08
think about it that also is like a nice interpretation it's like a mapping from image space to a label space yep good
37:15
how do you come up with the with each of the numbers in the column Vector oh I made that up sorry how do you get them
37:22
from the red and blue values sorry how do you get them from the red green and blue Valu
37:28
uh yeah thank you so uh good question so this image is a threedimensional uh array where we have all these channels
37:33
you just uh stretch it out so all the you just stretch it out in whatever way you like say you stack the red green and
37:40
blue portions side by side that's one way you just stretch it out in whatever way you like but in a consistent way
37:45
across all the images you figure out a way to serialize in which way you want to read off the pixels and you stretch
37:50
them out into a column so then when you have 12 values uh for four pixel image yeah okay
37:57
good Point um okay yeah so let's say we
38:02
have a four pixel grayscale image which is this is a terrible example thank you you're right thank you I didn't want to
38:08
confuse people especially because someone pointed out to me later after I made this figure that red green and blue are the color channels but here the red
38:13
green and blue correspond to classes so this is a complete uh screw up on my part so I apologize not color channels
38:20
nothing to do with color channels just three different colored classes sorry about that okay go ahead so uh we have a
38:28
different size of image like 40 * 600 time something like that do you put a sort of zero inside
38:35
or yeah thank you so your question is what if my images have different sizes in my data set some could be small some
38:42
could be large exactly how do we make this all be a single sized column Vector
38:47
uh the answer is you always we always resize images to be basically the same size we can't easily deal with different
38:53
sized images uh or we can and we might go into that in a in later but but the simplest thing to think of is just
38:59
resize every single image to the exact same uh size is the simplest thing because we want to ensure that all of
39:04
them are kind of uh comparable of the same stuff so that we can make these columns and we can analyze statistical
39:10
patterns that are aligned in the image space um yeah uh in fact
39:16
state-of-the-art methods the way they actually work on this is they always work on Square images so if you have a very long image these methods will
39:21
actually work worse because many of them what they do is just squash it that's what we do still works fairly well
39:27
uh so yeah if you have very long like panr images and you try to put that somewhere on like some online service chances are it might work worse because
39:34
they'll probably when they put it through a compet they will make it a square because these componets always work on squares uh you can make them work on
39:41
anything but that's just in practice what happens usually uh any other questions or yeah
39:46
it's like assigning a score to each test image for each class uh yep so you're
39:52
interpreting the W the classifier yeah yeah so each image gets mapped through this uh setting okay anyone anyone else
39:59
would like to interpret this or okay great so another way to actually put it one way that I didn't hear but it's also
40:05
a relatively nice way of looking at it is that basically every single score is just a weighted sum of all the pixel
40:12
values in the image and these weights are we get to choose those eventually but it's just a giant weighted sum it's
40:17
really all it's doing is it's counting up colors right it's counting up colors at different spatial positions so uh one
40:24
way to one way that was brought up in terms of how we can inter interpret this W classifier to make it concrete is that
40:30
it's kind of like a bit like a template matching thing so here what I've done is I've trained a classifier and I haven't
40:35
shown you how to do that yet but I trained my weight Matrix W and then I'll come back to this in a second I'm taking
40:41
out every single one of those rows that we've learned every single classifier and I'm reshaping it back to an image so
40:46
that I can visualize it okay so I'm taking it originally just a giant row of 372 numbers I reshape it back to the
40:53
image to undo the Distortion I've done and then I have all these templates and so for example what you see here is
40:59
that plane it's like a blue blob here the reason you see blue blob is that if you looked at the color channels of this
41:06
plain template you'll see that in the blue Channel you'll have lots of positive weights because those positive weights if they see blue values then
41:13
they interact with those and they get a little contribution to the score so this PL classifier is really just counting up
41:19
the amount of blue stuff in the image across all these spatial locations and if you look at the red and the green channel for the plane classifier you
41:25
might find zero values or even negative values right so that's the plain classifier and then we have classifiers
41:31
for all these other images so say a frog you can almost see the template of the Frog there right we're looking for some
41:37
green stuff is green stuff has positive weights in here and then we see some brown stuff is things on the side right
41:43
so if that gets put over an image and do producted you'll get a high score um one thing to note here is that
41:50
look at this the car classifier that's not a very like nice template of a car
41:56
also here the horse look looks a bit weird what's up with that why is the car looking weird and why is the horse
42:01
looking weird yeah some horses are facing left and some are facing right we end up with this where it looks like a horse is two
42:08
heads both basically that's what's going on in the data the horses some are facing left somewh right and this
42:14
classifier really it's not very powerful classifier and it has to combine the two modes it has to do both things at the
42:19
same time so you end up with this two-headed horse in there and you can in fact say that just from this result
42:25
there's probably more left facing horses in far than right because it's stronger there uh also for car right we can have
42:31
a car like 45° tilted left or right or front and this classifier here is the
42:37
optimal way of mixing across like merging all those modes into a single template because that's what we're
42:42
forcing it to do once we're actually doing com nuts and neural networks they don't have this downside they can actually have in principle they can have
42:49
a template for this car or that car or that car and combine across them we're giving them more power to actually carry out this classification more properly
42:56
but for now we're constrained by this question if you only have one image of a horse or only a couple you rotate it
43:03
yourself bu different ways and in certain ways so thatass would over to
43:09
one orientation uh yes what you're referring to I think is something we call data augmentation so at training time we
43:15
would not be taking just uh exact images but we'll be jittering them stretching them spewing them and we'll be piping
43:21
all of that in that's going to become a huge part of getting comat to work very well uh so yeah so we'll be doing a huge
43:26
amount of that stuff uh for every single training example we're going to hallucinate many other training examples of shifts and rotates
43:33
and SKS and that works much better go ahead uh how do these templates differ from just taking the average pixel value
43:41
for each class uh how do these templates uh change taking the average pixel value
43:47
took all the images for a plane took the average all the I see so you want to uh
43:54
explicitly set a template and the way you'll set the temp is you'll take the average across all the images and that becomes your template sure I'm just
44:01
wondering yeah how those two approaches would
44:07
differ um so this classifier it finds it
44:12
would do something similar I would guess it would work worse uh because the linear classifier when you look at its
44:18
mathematical form and really what it optimizes for I don't think it would have a minimum at what you described and just the mean of the images uh but that
44:24
would be like a intuitively decent heuristic to perhaps set the weights in the initialization or something like
44:30
that classier that does that like something yeah there's something related to it uh yeah yeah but we might even go
44:37
into that LDA probably is what you're referring to there's several several things okay yeah uh don't you think it
44:44
would be much better if we do it with gray scale images because let's say we got a car that was like yellowish right
44:53
but our template image highlights red right so wouldn't it be better if you
44:58
race yeah so that's a good point so there are cars have many different colors and here this happened to pick up on red which is saying that there's
45:05
probably more red cars in the data set and uh it might not work for yellow in fact yellow cars might be frog for this
45:11
classifier right um so this thing just does not have capacity to do all of that which is why it's not powerful enough it
45:17
can't capture all these different modes correctly and so this will just go after the numbers there's more red cars that's
45:23
where it will go uh if this was grayscale I'm not sure if that would work better I'll come back
45:28
to that actually in a bit go ahead if you have a training set with more examples for cash would that affect the
45:33
buyas higher and if so would that affect the the test uh yeah so you might expect as I
45:39
mentioned for unbalanced data sets what you might expect um not
45:44
exactly you what you might expect if you have lots of cats is that the cat bias would be higher because this class this
45:51
this classifier is just used to spewing out large numbers uh based on the loss but uh we have to go into loss fun to
45:57
exactly see how that will play out uh so it's hard to say right now um okay uh
46:03
another interpretation of the linear classifier that also someone else pointed out that I'd like to point out is uh you can think of these images as
46:09
very high dimensional points in a 372 dimensional space right in the 372 pixel
46:15
space dimensional pixel space every image is a point and these linear classifiers are describing these
46:21
gradients across the 372 dimensional space these scores are this uh gradient of negative to positive along some
46:27
linear Direction across the space and so for example here for a car classifier I'm taking the first row of w which is
46:33
the car class and the line here is indicating the zero level set of that classifier in other words that along
46:40
that line the car classifier has zero score so the car classifier there has zero and then the arrow is indicating
46:46
the direction along which it will color the space uh with more and more carness
46:52
of score similar we have these three different classifiers in this example they will correspond to these gradients
46:58
with a particular level set and they're basically trying to go in you have all these points they are in the space and
47:05
these linear classifiers we initialize them randomly so this car classifier would have its level set at random and then you'll see when we actually do the
47:11
optimization as we optimize this will start to shift turn and it will try to isolate the car class and we like it's
47:19
really fun to watch these classifiers train because it will it will rotate it will snap in for the car class and it'll start to jiggle and it will try to like
47:25
separate out all the cars from all the we got all the non cars it's really amusing to watch so that's another way
47:31
of interpreting that someone has brought up okay so here's a question for you given all these interpretations what
47:36
would be a very hard test set given how the linear classifier Works what would you expect to work really just really
47:42
not well with a linear
47:49
classifier concurrent circles sorry concurrent circles concurrent circles so
47:54
your classes are what are your CL classes exactly so you have a red class in the middle that's like Circle or like
48:00
a blob of red classes and then around it you have a blob of blue classes oh I see so you're in okay so what you're
48:06
describing is in this interpretation of space if your images in one class would be in a blob and then your other class
48:13
is like around it so I'm not sure exactly what that would look like if you actually visualize it in in a pixel space uh but yes you're right in that
48:20
case linear classifier would not be able to separate out those but what about in terms of like uh what would the images look like you would look at this data
48:27
set of images and you clearly say that linear classifier will probably not do very well
48:32
here yeah go ahead you want to separate like scooters for motorcycles basically or something like
48:38
that because I think somebody else was asking about the averaging of the pictures and that that made me think if
48:44
you're doing like ordinary Le squares what you're basically doing is maximizing projections of of all the X's
48:50
onto your row space so if you look at training X's of scooters and training X's of motorcycles they're going to
48:57
effectively yield the same like centroid for the template and then you're going to get the same situation describing
49:04
where you have concentric circles in yourens space mhm yep yep so that's a pretty good one yeah good maybe the
49:10
negative images like uh CH you know subtracting 255 whatever the color is
49:17
you know like so you want one Clause to be say you take all the airplanes and then you
49:23
you want to switch like an airplane to a human if it were the negative like film negative image of a oh I see it would it
49:30
would be actually the lowest scoring oh I see I see so you're you're pointing out that if I took that image of an airplane class and I have a trained
49:36
classifier and then I do a negative of it negative image of that classifier you'd still see the edges and you you'll
49:41
say okay that's an airplane obviously by the shape but for linear classifier all the colors would be exactly wrong and so
49:47
the linear classifier would hate that airplane so yeah good example good you take the same exact image and you
49:54
translate it or scale it differently and you move it to different places are rotated for each class you have several of these I'd expect that to perform I
50:02
see so uh you're saying that we take one thing say like dogs but then so what you're referring to is say you have dogs
50:08
one class is dogs in the center and one class is dogs in the on the right and you think that that would be uh yeah I'm
50:13
saying even if it's a picture of the same exact dog same pose and everything you just move it to the right up down or
50:19
you scale it to different sizes and move it around so would that be a problem if so one class is dogs in the center and
50:25
one class is dogs in the right but otherwise white background or something like that would that be a
50:30
problem it wouldn't be a problem why wouldn't it be a problem transformation uh it's an aine
50:37
transformation a linear transformation what you areel I guess that the problem would be if you work an image you have a
50:45
set of images like dogs and dogs work or ex skewed in some nonlinear way I
50:51
see right so you're saying that maybe a more difficult thing would be if you have dogs that are warped in some funny
50:57
ways according to class why wouldn't it be a problem if you actually could a linear classifier do something in the
51:02
center and something on the right does it actually have an understanding of a spatial layout that' actually be fine
51:07
right that would be relatively easy because you would have positive weights in the middle oh Sor suggesting oh sorry
51:15
okay maybe I'm misunderstood I'm sorry okay another one classifying different types of P names the author be hard
51:22
because they use Bunch different colors and different Set uh yeah possibly yeah so I think many of you are kind of
51:27
understanding the the main point is uh yeah so this is really really what it's doing well I'm skipping ahead here
51:33
really what this is doing is it's counting up uh counting up colors in spatial positions anything that messes
51:38
with this will be really hard actually to go back to your point if you had a grayscale data set by the way that would work not very well with L linear
51:45
classifiers would probably not work if you took CFR 10 and you make it all grayscale then doing the exact cartan
51:52
classification but on grayscale images would probably work really terribly because you can't pick up on the colors you have to pick up on these textures
51:58
and fine details now and you just can't localize them because they could be at arbitrary positions you can't
52:04
consistently count across it um so yeah that would be kind of a disaster uh another example would be different
52:09
textures if you have say all of your textures are blue but these textures could be uh different types then this
52:16
doesn't really like say these textures could be different types but they can be spatially invariant and that would be terrible terrible for all your
52:22
classifier as well okay good uh so just to remind you I think uh
52:29
nearly there um we defin this linear function so with a specific case of w
52:34
we're looking at some test images we're getting some scores out and just looking forward where we're headed now is with
52:40
some setting of W's we're getting some scores for all these images and so for example with this setting of w in this
52:46
image we're seeing that the cat score is 2.9 but there are some classes that got a higher score like dog so that's not
52:52
very good right but some classes have negative scores which is uh good for this image so this is a kind of a medium result for
52:59
this weights for this image and here we see that the car class which is correct for there has the highest score which is
53:05
good right so this setting of w worked well on this image here we see that the for class is a very low score so w
53:12
worked terribly on that image so where we're headed now is we're going to Define what we call a loss function and
53:19
this loss function will quantify this intuition of what we consider good or bad right now we're just eye boiling
53:24
these numbers and saying what's good what's bad we have to actually write down a mathematical expression that tells us exactly like this setting of w
53:31
across our test set is 12.5 bad or 12 whatever bad or 1.0 bad because then
53:38
once we have it defined specifically we're going to be looking for W's that minimize the loss and it will be set up
53:43
in such a way that when you have a loss of very low numbers like say even zero then you're correctly classifying all
53:49
your images um but if you have a very high loss then everything is messed up and W is not good at all so we're going to
53:56
Define a loss function and then we're going to look for different W's that actually do very well across all of it
Loss functions
54:02
so that's roughly what's coming up we'll Define loss function which is a quantify a way to quantify how bad each W is on
54:09
our data set the loss function is a function of your entire training set and your weights we don't have um control
54:17
over the training set but we have control over the weights then we're going to look at the process of optimization how do we efficiently find
54:22
the set of Weights W that works across all the images and gives us a very low loss and then eventually what we'll do
54:29
is we'll go back and we'll look at this expression the linear classifier that we saw and we're going to start meddling with the function f here so we're going
54:36
to extend F to not be that simple in your expression but we're going to make it slightly more complex we'll get a neural network out and then we'll make
54:42
it slightly more complex and we'll get a convolutional network out but otherwise the entire framework will stay unchanged
54:47
all the time we'll be Computing these scores this functional form will be changing but we're going from image to some set of scores uh through some
54:55
function and we'll make it more elaborate over time and then we're identifying some loss function and we're
55:00
looking at what weights what parameters are giving us very low loss and that's the setup will we working with uh going
55:05
forward so next class we'll look into loss functions and then we'll go towards neural networks and comets so um I guess
55:12
this is my last slide so I can take uh any last questions and then can you
55:18
explain the advantage of this itative approach uh sorry sorry uh sorry I
55:27
didn't hear so I was saying why are we
55:36
doing why are we doing alterative approach in the optimization so sometimes in optimization settings you can have for
55:42
um these iterative approaches are basically the way this will work we'll we'll always start off with a random W
55:48
uh so that will give us some loss and then we we don't have a process of finding right away the best set of Weights what we do have a process for is
55:55
iterative slightly improving the weights so what we'll see is we'll look at the L function and we'll find the gradient in
56:01
in the parameter space and we'll March down so what we do know how to do is how do we slightly improve a set of Weights
56:07
we don't know how to do the problem of just find the best set of weights right away we don't know how to do that because especially when these functions
56:13
are very complex like say entire comets it's a huge landscape of it's just a very uh intractable problem is that your
56:20
question I'm not sure I okay thank you um
56:27
yeah so how do you deal with that uh color problem how do we deal with the color problem
56:33
yeah since we had a high bias of red cars oh so okay so uh so here we saw
56:39
that the linear classifier for a car was this red template for a car a neural network basically what we'll do is we'll
56:45
be will you can look at it as stacking linear classifiers to some degree so what it'll end up doing is it will have
56:50
all these little templates really for red cars yellow cars green cars whatever cars going this way or that way or that
56:56
way there will be a neuron assigned to detecting every one of these different modes and then they will be combined across them on a second layer so
57:02
basically you'll have these neurons looking for different types of cars and then the next neuron will be just like okay I just take a wait at some of you
57:08
guys and I'm just doing an or operation over you and then we can detect cars in all of their modes and all of their
57:14
positions if that makes sense uh so that's roughly how it will work makes sense okay awesome
57:25
okay this